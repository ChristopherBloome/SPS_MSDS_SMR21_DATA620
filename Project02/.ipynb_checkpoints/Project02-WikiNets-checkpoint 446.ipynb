{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "987da827-1567-4f01-806f-687b66c830b4",
   "metadata": {},
   "source": [
    "<b>Web Analytics DATA 620 - Project 02</b>\n",
    "\n",
    "<b>Assignment: “Wiki Publishing”</b>\n",
    "\n",
    "<b>Group - Chris Bloome / Mustafa Telab / Vinayak Kamath</b>\n",
    "\n",
    "<b>Date - 24th June 2021</b>\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "Identify a large 2-node network dataset—you can start with a dataset in a repository.  Your data should meet the criteria that it consists of ties between and not within two (or more) distinct groups.\n",
    "Reduce the size of the network using a method such as the island method described in chapter 4 of social network analysis.\n",
    "What can you infer about each of the distinct groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c847e-9c1d-4ba9-b7e0-83c62ab97742",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460459d2-0644-4a9a-8e00-82b82dff0578",
   "metadata": {},
   "source": [
    "<b>Wikipedia User Publishing</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c592a-0fec-4800-808f-8fe55a34b19b",
   "metadata": {},
   "source": [
    "The source of the data is http://networkrepository.com/ia-wiki-user-edits-page.php#\n",
    "\n",
    "The data is a collection of edges that represent users and wikipedia pages; while the edges represent a edit events.\n",
    "\n",
    " - User\n",
    " - Web page\n",
    " - Weight\n",
    " - Time Stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2782f8-bd35-4ee5-960f-fd731a2fcb6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51061f0",
   "metadata": {},
   "source": [
    "<b>Plan</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b37ee1",
   "metadata": {},
   "source": [
    " 1. Import and explore the edge data.\n",
    " 2. Identify opportunities to filter out irrelevant data.\n",
    " 3. Incorporate the \"island\" method to isolate the meaningful clusters\n",
    " 4. Apply appropriate bipartite weights to visualize important edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679f476",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6e360",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc62764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import networkx as nx\n",
    "import networkx.algorithms.bipartite as bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d836a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataset\n",
    "df = pd.read_csv('ia-wiki-user-edits-page.edges', sep = ' ',header = None, names = ['source','target','weight','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6af6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert timestamp to datetime format\n",
    "df['date'] = [datetime.utcfromtimestamp(v).strftime('%Y-%m-%d') for v in df['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = pd.to_datetime(df['date']).dt.strftime(\"%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98315c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag nodes to distiguish later\n",
    "df['source'] = ['u'+ str(v) for v in df['source']]\n",
    "df['target'] = ['p'+ str(v) for v in df['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eded02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to include only latest years data\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "maxyear = df['date'].dt.year.max()\n",
    "df = df[df['date'].dt.year == maxyear]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With repeat edits common, we can roll the values to a single edge and assign the count to the 'weight' attribute\n",
    "df_grouped = df.groupby(['source','target']).aggregate({('weight'):np.sum}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc600563",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "With other a million edge, we will examine the data set in a pandas dataset before reading into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the degrees for the users and pages\n",
    "u_count = df_grouped['source'].value_counts(ascending=True).rename_axis('source').reset_index(name='pages')\n",
    "p_count = df_grouped['target'].value_counts(ascending=True).rename_axis('target').reset_index(name='users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36059e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5ac43",
   "metadata": {},
   "source": [
    "We notice that some users has degrees in the hundreds of thousands.  It is understood that WIkipedia enlists the help of Bots to maintain many of the pages.  Lets examine the daily frequency of edits to establish a humanly feasible cut-off so we can filter out the bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency_step = df.groupby(['source','date']).aggregate({('weight'):np.sum}).sort_values('weight').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency = df_frequency_step.groupby(['source']).aggregate({('weight'):np.average}).sort_values('weight').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency.columns = ['source', 'frequency']\n",
    "df_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_frequency['frequency'], bins = 100 )\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cdaab0",
   "metadata": {},
   "source": [
    "The frequency distribition shows that the bots are easily distriguishable from just a few multiples of std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b367fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stats = df_frequency[\"frequency\"].describe().round(2)\n",
    "print('Edit Daily Frequency Statisitics')\n",
    "print(freq_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(u_count['pages'], bins = 10)\n",
    "plt.title('Pages Per User Statisitics')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05040b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = u_count[\"pages\"].describe().round(2)\n",
    "print('Pages Per User Statisitics')\n",
    "print(user_stats, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82678bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_count['users'], bins = 30)\n",
    "plt.title('Users Per Page Statisitics')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stats = p_count[\"users\"].describe().round(2)\n",
    "print('Users Per Page Statisitics')\n",
    "print(page_stats, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.merge(df_grouped, u_count, how=\"left\", on=\"source\")\n",
    "df_grouped = pd.merge(df_grouped, p_count, how=\"left\", on=\"target\")\n",
    "df_grouped = pd.merge(df_grouped, df_frequency, how=\"left\", on=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_grouped))\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d48dd",
   "metadata": {},
   "source": [
    "## Island Method\n",
    "While still in the pandas format, we will enlist the island method by filtering the edges to include only most active users and pages; while simultaniuosly removing the uber active users, which we have already determined are bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e52981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set \"water levels\"\n",
    "freq_std = freq_stats['std']\n",
    "user_min = user_stats['75%']\n",
    "page_min = page_stats['75%']\n",
    "#apply filter\n",
    "df_island = df_grouped[(df_grouped['pages']>user_min)  & (df_grouped['users']>page_min) & (df_grouped['frequency'] < (freq_std * 5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c773411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('From the above steps, we have reduced a list of almost', len(df), 'edges to', len(df_island)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdde313",
   "metadata": {},
   "source": [
    "## Establish initial graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = nx.from_pandas_edgelist(df_island, source='source', target='target', edge_attr=[\"weight\"], create_using = nx.DiGraph(), edge_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81afaf",
   "metadata": {},
   "source": [
    "A succesful diameter result test us we have a connected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdaeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [v for v in S.nodes() if v[0]== 'u']\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [v for v in S.nodes() if v[0]== 'p']\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P = bipartite.projected_graph(S.to_undirected(), pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c60e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = bipartite.projected_graph(S.to_undirected(), users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb18349",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite.collaboration_weighted_projected_graph(B, [0, 2, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edad031",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.degree_centrality(U).items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dict(U.to_undirected().degree()).items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b280a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dict(nx.eigenvector_centrality(U)).items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "density = nx.density(U)\n",
    "print(density)\n",
    "print(nx.is_connected(U.to_undirected()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_degrees = dict(U.degree()).values()\n",
    "plt.hist(U_degrees, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifiction of code clock found on Social Network Analysis for Startups, pg64 \n",
    "def trim_nodes(g, weight=1):\n",
    "    nodes = []\n",
    "    for n in g.nodes():\n",
    "        if g.degree(n) > weight:\n",
    "            nodes.append(n)\n",
    "    G2 = g.subgraph(nodes)\n",
    "    return G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90576317",
   "metadata": {},
   "outputs": [],
   "source": [
    "U2 = trim_nodes(U,100)\n",
    "print(nx.info(U2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c6018",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "U2_degrees = dict(U2.degree()).values()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "nx.draw_spring(U2 ,alpha =.7,width = .01, arrows=False , node_size = [v*.3 for v in U2_degrees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = bipartite.overlap_weighted_projected_graph(S.to_undirected(),list(U2.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_degrees = dict(W.degree()).values()\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "nx.draw_spring(W ,alpha =.7,width = .01, arrows=False , node_size = [v*.3 for v in W_degrees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = trim_nodes(W,100)\n",
    "print(nx.info(W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81537409",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2_stats = pd.Series(W2_weights).describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2_degrees = dict(W2.degree()).values()\n",
    "W2_weights = [v*10 for v in nx.get_edge_attributes(W2, 'weight').values()]\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "nx.draw(W2 ,alpha =.7,width = W2_weights, arrows=False , node_size = [v*.3 for v in W_degrees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab703122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifiction of code clock found on Social Network Analysis for Startups, pg64 \n",
    "def trim_edges(g, weight=1):\n",
    "    l = []\n",
    "    g2 = g.copy(as_view=False)\n",
    "    for f, to, edata in g.edges(data=True):\n",
    "            if edata['weight'] < weight:\n",
    "                l.append((f,to))\n",
    "    g2.remove_edges_from(l)\n",
    "    g3 = nx.subgraph(g2, list(nx.connected_components(W3))[0])\n",
    "    return g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = trim_edges(W2, W2_stats['50%']/3)\n",
    "print(nx.info(W3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(nx.connected_components(W3))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3_degrees = dict(W3.degree()).values()\n",
    "W3_weights = [v*5 for v in nx.get_edge_attributes(W3, 'weight').values()]\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "nx.draw_spring(W3 ,alpha =.7,width = W3_weights, arrows=False , node_size = [v*.5 for v in W3_degrees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__author__ = \"\"\"\\n\"\"\".join(['Maksim Tsvetovat <maksim@tsvetovat.org',\n",
    "'Drew Conway <drew.conway@nyu.edu>',\n",
    "'Aric Hagberg <hagberg@lanl.gov>'])\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_hc(G, t=1.0):\n",
    "    \"\"\"\n",
    "    Creates hierarchical cluster of graph G from distance matrix\n",
    "    Maksim Tsvetovat ->> Generalized HC pre- and post-processing to work on labelled graphs\n",
    "    and return labelled clusters\n",
    "    The threshold value is now parameterized; useful range should be determined\n",
    "    experimentally with each dataset\n",
    "    \"\"\"\n",
    "    \"\"\"Modified from code by Drew Conway\"\"\"\n",
    "    \n",
    "    ## Create a shortest-path distance matrix, while preserving node labels\n",
    "    labels=G.nodes()\n",
    "    path_length=nx.all_pairs_shortest_path_length(G)\n",
    "    distances=numpy.zeros((len(G),len(G)))\n",
    "    i=0\n",
    "    for u,p in path_length:\n",
    "        j=0\n",
    "        for v,d in p.items():\n",
    "            distances[i][j]=d\n",
    "            distances[j][i]=d\n",
    "            if i==j: distances[i][j]=0\n",
    "            j+=1\n",
    "        i+=1\n",
    "        \n",
    "    # Create hierarchical cluster\n",
    "    Y=distance.squareform(distances)\n",
    "    Z=hierarchy.complete(Y) # Creates HC using farthest point linkage\n",
    "    \n",
    "    # This partition selection is arbitrary, for illustrive purposes\n",
    "    membership=list(hierarchy.fcluster(Z,t=t))\n",
    "    \n",
    "    # Create collection of lists for blockmodel\n",
    "    partition=defaultdict(list)\n",
    "    for n,p in zip(list(range(len(G))),membership):\n",
    "        if p>=0:\n",
    "            partition[p].append(labels[n])\n",
    "    return list(partition.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e05031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters = create_hc(U2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
