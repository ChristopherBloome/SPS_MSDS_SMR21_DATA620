{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "987da827-1567-4f01-806f-687b66c830b4",
   "metadata": {},
   "source": [
    "<b>Web Analytics DATA 620 - Project 02</b>\n",
    "\n",
    "<b>Assignment: “Wiki Publishing”</b>\n",
    "\n",
    "<b>Group - Chris Bloome / Mustafa Telab / Vinayak Kamath</b>\n",
    "\n",
    "<b>Date - 24th June 2021</b>\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "Identify a large 2-node network dataset—you can start with a dataset in a repository.  Your data should meet the criteria that it consists of ties between and not within two (or more) distinct groups.\n",
    "Reduce the size of the network using a method such as the island method described in chapter 4 of social network analysis.\n",
    "What can you infer about each of the distinct groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c847e-9c1d-4ba9-b7e0-83c62ab97742",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460459d2-0644-4a9a-8e00-82b82dff0578",
   "metadata": {},
   "source": [
    "<b>Wikipedia User Publishing</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c592a-0fec-4800-808f-8fe55a34b19b",
   "metadata": {},
   "source": [
    "The source of the data is http://networkrepository.com/ia-wiki-user-edits-page.php#\n",
    "\n",
    "The data is a collection of edges that represent users and wikipedia pages; while the edges represent a edit events.\n",
    "\n",
    " - User\n",
    " - Web page\n",
    " - Weight\n",
    " - Time Stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897189f-eb3a-46fb-8d61-b99e4d427676",
   "metadata": {},
   "source": [
    "<b>Analysis Plan</b>\n",
    "1. We will load the csv data into a temp object and then pull the distinct nodes and its characteristics and the linkages between the nodes.\n",
    "  - The Nodes represent the User, the bikes,  and the Stations. The User node having characterisitics of Type, Gender, and Year of birth and the Station node having the characteristics of  latitude and logitude. The Bike node has no characteristics other then a unique Id.\n",
    "  - The linkages represet the trips between start station and stop station.\n",
    "\n",
    "2. Degree centrality is defined as the Number of incoming links to a node. We can check the station having the maximum number of trips to it and we can further to see if the same station has the maximum degree centrality for each gender type and/or user types. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2782f8-bd35-4ee5-960f-fd731a2fcb6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc62764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1053410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ia-wiki-user-edits-page.edges', sep = ' ',header = None, names = ['source','target','weight','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6af6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = [int(datetime.utcfromtimestamp(v).strftime('%H')) for v in df['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9bf99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [int(datetime.utcfromtimestamp(v).strftime('%m')) for v in df['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdc915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour']= pd.Series(hour)\n",
    "df['month']= pd.Series(month)\n",
    "df['source'] = ['u'+ str(v) for v in df['source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a0ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = ['p'+ str(v) for v in df['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbaa36ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8998641\n",
      "  source target  weight        time  hour  month\n",
      "0     u1     p1       1  1039680411     8     12\n",
      "1     u1     p1       1  1039680680     8     12\n",
      "2     u1     p1       1  1039680886     8     12\n",
      "3     u2     p1       1  1040932914    20     12\n",
      "4     u3     p1       1  1052273037     2      5\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(['source','target']).aggregate({('weight'):np.sum,('hour'):np.average}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ddb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497028c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count = df_grouped['source'].value_counts(ascending=True).rename_axis('source').reset_index(name='pages')\n",
    "p_count = df_grouped['target'].value_counts(ascending=True).rename_axis('target').reset_index(name='users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(u_count['pages'], bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_count['users'], bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07503316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.merge(df_grouped, u_count, how=\"left\", on=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22791e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.merge(df_grouped, p_count, how=\"left\", on=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_grouped))\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd99d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = nx.from_pandas_edgelist(df_grouped[df_grouped['pages']>1000], source='source', target='target', edge_attr=[\"weight\", \"hour\"], create_using = nx.DiGraph(), edge_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebccdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifiction of code clock found on Social Network Analysis for Startups, pg64 \n",
    "def trim_nodes(g, weight=1):\n",
    "    nodes = []\n",
    "    for n in g.nodes():\n",
    "        if g.degree(n) > weight:\n",
    "            nodes.append(n)\n",
    "    G2 = g.subgraph(nodes)\n",
    "    return G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90576317",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = trim_nodes(G,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6023fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(S2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"\"\"\\n\"\"\".join(['Maksim Tsvetovat <maksim@tsvetovat.org',\n",
    "'Drew Conway <drew.conway@nyu.edu>',\n",
    "'Aric Hagberg <hagberg@lanl.gov>'])\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "def create_hc(G, t=1.0):\n",
    "    \"\"\"\n",
    "    Creates hierarchical cluster of graph G from distance matrix\n",
    "    Maksim Tsvetovat ->> Generalized HC pre- and post-processing to work on labelled graphs\n",
    "    and return labelled clusters\n",
    "    The threshold value is now parameterized; useful range should be determined\n",
    "    experimentally with each dataset\n",
    "    \"\"\"\n",
    "    \"\"\"Modified from code by Drew Conway\"\"\"\n",
    "    ## Create a shortest-path distance matrix, while preserving node labels\n",
    "    labels=G.nodes()\n",
    "    path_length=nx.all_pairs_shortest_path_length(G)\n",
    "    distances=numpy.zeros((len(G),len(G)))\n",
    "    i=0\n",
    "    for u,p in path_length:\n",
    "        j=0\n",
    "        for v,d in p.items():\n",
    "            distances[i][j]=d\n",
    "            distances[j][i]=d\n",
    "            if i==j: distances[i][j]=0\n",
    "            j+=1\n",
    "        i+=1\n",
    "    # Create hierarchical cluster\n",
    "    Y=distance.squareform(distances)\n",
    "    Z=hierarchy.complete(Y) # Creates HC using farthest point linkage\n",
    "    # This partition selection is arbitrary, for illustrive purposes\n",
    "    membership=list(hierarchy.fcluster(Z,t=t))\n",
    "    # Create collection of lists for blockmodel\n",
    "    partition=defaultdict(list)\n",
    "    for n,p in zip(list(range(len(G))),membership):\n",
    "        partition[p].append(labels[n])\n",
    "    return list(partition.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e05031",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = create_hc(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daa03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_length=nx.all_pairs_shortest_path_length(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df52fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_length[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
