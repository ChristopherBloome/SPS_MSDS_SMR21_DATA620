{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "987da827-1567-4f01-806f-687b66c830b4",
   "metadata": {},
   "source": [
    "<b>Web Analytics DATA 620 - Project 02</b>\n",
    "\n",
    "<b>Assignment: “Wiki Publishing”</b>\n",
    "\n",
    "<b>Group - Chris Bloome / Mustafa Telab / Vinayak Kamath</b>\n",
    "\n",
    "<b>Date - 24th June 2021</b>\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "Identify a large 2-node network dataset—you can start with a dataset in a repository.  Your data should meet the criteria that it consists of ties between and not within two (or more) distinct groups.\n",
    "Reduce the size of the network using a method such as the island method described in chapter 4 of social network analysis.\n",
    "What can you infer about each of the distinct groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c847e-9c1d-4ba9-b7e0-83c62ab97742",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460459d2-0644-4a9a-8e00-82b82dff0578",
   "metadata": {},
   "source": [
    "<b>Wikipedia User Publishing</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c592a-0fec-4800-808f-8fe55a34b19b",
   "metadata": {},
   "source": [
    "The source of the data is http://networkrepository.com/ia-wiki-user-edits-page.php#\n",
    "\n",
    "The data is a collection of edges that represent users and wikipedia pages; while the edges represent a edit events.\n",
    "\n",
    " - User\n",
    " - Web page\n",
    " - Weight\n",
    " - Time Stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2782f8-bd35-4ee5-960f-fd731a2fcb6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc62764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import networkx as nx\n",
    "import networkx.algorithms.bipartite as bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d836a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ia-wiki-user-edits-page.edges', sep = ' ',header = None, names = ['source','target','weight','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6af6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = [int(datetime.utcfromtimestamp(v).strftime('%H')) for v in df['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [int(datetime.utcfromtimestamp(v).strftime('%m')) for v in df['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour']= pd.Series(hour)\n",
    "df['month']= pd.Series(month)\n",
    "df['source'] = ['u'+ str(v) for v in df['source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98315c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = ['p'+ str(v) for v in df['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(['source','target']).aggregate({('weight'):np.sum,('hour'):np.average}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count = df_grouped['source'].value_counts(ascending=True).rename_axis('source').reset_index(name='pages')\n",
    "p_count = df_grouped['target'].value_counts(ascending=True).rename_axis('target').reset_index(name='users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36059e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b367fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pages Per User Statisitics')\n",
    "print(u_count[ \"pages\"].describe()+ '\\n\"')\n",
    "print('Users Per Page Statisitics')\n",
    "print(p_count[ \"users\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.hist(u_count['pages'], bins = 10))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82678bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_count['users'], bins = 30)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.merge(df_grouped, u_count, how=\"left\", on=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.merge(df_grouped, p_count, how=\"left\", on=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_grouped))\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e52981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_island = df_grouped[(df_grouped['pages']>100)  & (df_grouped['users']>20)]\n",
    "len(df_island)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = nx.from_pandas_edgelist(df_island, source='source', target='target', edge_attr=[\"weight\", \"hour\"], create_using = nx.DiGraph(), edge_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdaeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(nx.connected_components(S.to_undirected())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [v for v in S.nodes() if v[0]== 'u']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89714b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "users[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c60e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = bipartite.projected_graph(S.to_undirected(), users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e292973",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_bipartite(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifiction of code clock found on Social Network Analysis for Startups, pg64 \n",
    "def trim_nodes(g, weight=1):\n",
    "    nodes = []\n",
    "    for n in g.nodes():\n",
    "        if g.degree(n) > weight:\n",
    "            nodes.append(n)\n",
    "    G2 = g.subgraph(nodes)\n",
    "    return G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90576317",
   "metadata": {},
   "outputs": [],
   "source": [
    "U2 = trim_nodes(U,600)\n",
    "print(nx.info(U2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(U2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"\"\"\\n\"\"\".join(['Maksim Tsvetovat <maksim@tsvetovat.org',\n",
    "'Drew Conway <drew.conway@nyu.edu>',\n",
    "'Aric Hagberg <hagberg@lanl.gov>'])\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_hc(G, t=1.0):\n",
    "    \"\"\"\n",
    "    Creates hierarchical cluster of graph G from distance matrix\n",
    "    Maksim Tsvetovat ->> Generalized HC pre- and post-processing to work on labelled graphs\n",
    "    and return labelled clusters\n",
    "    The threshold value is now parameterized; useful range should be determined\n",
    "    experimentally with each dataset\n",
    "    \"\"\"\n",
    "    \"\"\"Modified from code by Drew Conway\"\"\"\n",
    "    \n",
    "    ## Create a shortest-path distance matrix, while preserving node labels\n",
    "    labels=G.nodes()\n",
    "    path_length=nx.all_pairs_shortest_path_length(G)\n",
    "    distances=numpy.zeros((len(G),len(G)))\n",
    "    i=0\n",
    "    for u,p in path_length:\n",
    "        j=0\n",
    "        for v,d in p.items():\n",
    "            distances[i][j]=d\n",
    "            distances[j][i]=d\n",
    "            if i==j: distances[i][j]=0\n",
    "            j+=1\n",
    "        i+=1\n",
    "        \n",
    "    # Create hierarchical cluster\n",
    "    Y=distance.squareform(distances)\n",
    "    Z=hierarchy.complete(Y) # Creates HC using farthest point linkage\n",
    "    \n",
    "    # This partition selection is arbitrary, for illustrive purposes\n",
    "    membership=list(hierarchy.fcluster(Z,t=t))\n",
    "    \n",
    "    # Create collection of lists for blockmodel\n",
    "    partition=defaultdict(list)\n",
    "    for n,p in zip(list(range(len(G))),membership):\n",
    "        if p>=0:\n",
    "            partition[p].append(labels[n])\n",
    "    return list(partition.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e05031",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = create_hc(U2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
